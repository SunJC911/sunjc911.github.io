---
title: 深度学习推荐系统
description: 
date: 2023-04-22
categories:
 - Rec面试
tags:
 - Rec
 - 面试
excerpt_separator: <!--more--> 

---

**加油**

<!--more-->

## 问题

[常见损失函数](https://blog.csdn.net/yanyuxiangtoday/article/details/119788949)

[L1L2正则化](https://zhuanlan.zhihu.com/p/137073968)

[梯度下降求导_LR](https://blog.csdn.net/qq_35890572/article/details/107123956)

[极大似然估计](https://lulaoshi.info/machine-learning/linear-model/maximum-likelihood-estimation.html#%E6%A6%82%E7%8E%87)

[GBDT系列](https://blog.csdn.net/XiaoYi_Eric/article/details/80167968)

随机梯度下降，批梯度下降等梯度下降法的区别

特征缩放，特征归一化等方法

## 第一章 互联网的增长引擎——推荐系统

### 推荐系统的作用与意义

用户角度: 推荐系统解决在“信息过载”的情况下，用户如何高效获得感兴趣信息的问题。

公司角度:推荐系统解决产品能够最大限度地吸引用户、留存用户、增加用户黏性提高用户转化率的问题，从而达到公司商业目标连续增长的目的。需要注意的是，设计推荐系统的最终目标是达成公司的商业目标、增加公司收益这应是推荐工程师站在公司角度考虑问题的出发点。

### 推荐系统的架构

(1) 互联网企业的核心需求是“增长”，而推荐系统正处在“增长引擎”的核心位置(2) 推荐系统要解决的“用户痛点”是用户如何在“信息过载”的情况下高效地获得感兴趣的信息。

![1](https://sunjc911.github.io/assets/images/DeepRec/1.png)

### 推荐系统的模型部分

“**召回层**”一般利用高效的召回规则、算法或简单的模型，快速从海量的候选集中召回用户可能感兴趣的物品。
“**排序层（重点与研究重心）**”利用排序模型对初筛的候选集进行精排序。
“补充策略与算法层”，也被称为“再排序层**”，可以在将推荐列表返回用户之前，为兼顾结果的“多样性”“流行度”“新鲜度”等指标，结合一些补充的策略和算法对推荐列表进行定的调整，最终形成用户可见的推荐列表。

**模型权重**：**离线训练**的特点是可以利用全量样本和特征，使模型逼近全局最优点;**在线更新**则可以准实时地“消化”新的数据样本，更快地反映新的数据变化趋势，满足模型实时性的需求。

**评估模型效果**：离线评估和线上A/B测

## 第二章 前深度学习时代的重要算法

协同过滤、逻辑回归、因子分解机等传统推荐模型仍然凭借其可解释性强、硬件环境要求低易于快速训练和部署。

传统推荐模型是深度学习推荐模型的基础。

![2](https://sunjc911.github.io/assets/images/DeepRec/2.png)

### 协同过滤CF

“协同过滤”就是协同大家的反馈、评价和意见一起对海量的信息进行过滤从中筛选出目标用户可能感兴趣的信息的推荐过程。

![3](https://sunjc911.github.io/assets/images/DeepRec/3.png)

#### UserCF

##### 用户相似度计算

###### 余弦相似度

余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小。余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，这就叫"余弦相似性"。

![cos](https://sunjc911.github.io/assets/images/DeepRec/cos.png)

###### 皮尔逊相关系数

相比余弦相似度，皮尔逊相关系数通过使用用户平均分对各独立评分进行修正，减小了用户评分偏置的影响。

![p](https://sunjc911.github.io/assets/images/DeepRec/p.png)

![p2](https://sunjc911.github.io/assets/images/DeepRec/p2.png)

理论上，任何合理的“向量相似度定义方式”都可以作为相似用户计算的标准。

###### 最终结果的排序

![4](https://sunjc911.github.io/assets/images/DeepRec/4.png)

###### UserCF缺点

（1）用户数远大于物品数，用户相似度**矩阵开销大**，用户数增长时矩阵空间复杂度以n平方的速度增长，存储系统难以承受。

（2）用户历史数据**向量非常稀疏**，不适用于正反馈获取困难的场景。

#### ItemCF

构建物品相似度矩阵，相似度计算方法与UserCF相同。

···

(4) 利用物品相似度矩阵，针对目标用户历史行为中的正反馈物品，找出相似的Top k个物品，组成相似物品集合。
(5) 对相似物品集合中的物品，利用相似度分值进行排序，生成最终的推荐列表。在第5步中，如果一个物品与多个用户行为历史中的正反馈物品相似，那么该物品最终的相似度应该是多个相似度的累加，如(式2-5) 所示。·

![5](https://sunjc911.github.io/assets/images/DeepRec/5.png)

#### UserCF和ItemCF的应用场景

UserCF：社交、发现热点、跟踪热点、新闻

ItemCF：电商、视频推荐

#### 协同过滤的优缺点

优点：直观、可解释性强

缺点：不具备较强泛化能力，产生头部效应（热门商品容易和大量物品产生相似，冷门商品向量稀疏则反过来很少被推荐），处理稀疏向量的能力弱

### 矩阵分解MF

针对CF的头部效应明显、泛化能力较弱的问题。在CF**共现矩阵的基础**上，加入**隐向量**的概念，加强模型处理稀疏矩阵的能力，针对性地解决了协同过滤存在的主要问题。

矩阵分解算法则期望为每一个用户和视频生成一个隐向量，将用户和视频定位到隐向量的表示空间上，距离相近的用户和视频表明兴趣特点接近，在推荐过程中，就应该把距离相近的视频推荐给目标用户。任意用户与物品之间都可以得到预测分。

![6](https://sunjc911.github.io/assets/images/DeepRec/6.png)

用户和物品的隐向量通过CF的共现矩阵得到。

![MF](https://sunjc911.github.io/assets/images/DeepRec/MF.png)

mxn分解为mxk和kxn。k小隐向量小泛化能力高，反之则反。

![MF2](https://sunjc911.github.io/assets/images/DeepRec/MF2.png)

#### 矩阵分解的求解过程

##### 特征值分解ED

只适用于方阵，这里不行。

##### 奇异值分解SVD

![MF3](https://sunjc911.github.io/assets/images/DeepRec/MF3.png)

完美解决MF但有缺陷：

（1）要求共现矩阵稠密。共现矩阵稀疏，若要用需填充。

（2）计算复杂度O（mn平方）

##### 梯度下降GD（重要）

![GD](https://sunjc911.github.io/assets/images/DeepRec/GD.png)

##### 正则化

![reg](https://sunjc911.github.io/assets/images/DeepRec/reg.png)

q=1 L1正则化；2 L2正则化

对于加入了正则化项的损失函数来说，模型权重的值越大，损失函数越大。梯度下降是**朝着损失小的方向**发展的，因此正则化项其实是希望在尽量不影响原模型与数据集之间损失的前提下，**使模型的权重变小**，权重的减小自然会让模型的输出波动更小，从而达到**让模型更稳定**的目的。

可以加上偏差系数修正预测分数。

![7](https://sunjc911.github.io/assets/images/DeepRec/7.png)

##### 矩阵分解的优缺点

优点：泛化能力强，缓解数据稀疏问题；空间复杂度低（n+m）k；更好的扩展性和灵活性。

缺点：没发使用用户物品上下文特征。

### 逻辑回归LR

CF和MF利用相似度进行推荐。LR将推荐问题看作**分类问题**，预测正样本的概率对物品进行排序，转化为**点击率（Click Through Rate, CTR）**预估问题。综合利用用户物品上下文等特征。利用“感知机MLP”这种NN的基本单元，是深度学习的基础。

各特征加权和再施以sigmod函数

离散转为数值型特征向量，训练，得到点击物品的概率，根据概率排序进行推荐。

#### 数学形式

![LR1](https://sunjc911.github.io/assets/images/DeepRec/LR1.png)

![LR2](https://sunjc911.github.io/assets/images/DeepRec/LR2.png)

![LR3](https://sunjc911.github.io/assets/images/DeepRec/LR3.png)

![LR4](https://sunjc911.github.io/assets/images/DeepRec/LR4.png)

#### 权重训练——梯度下降法

目的是对目标函数求导，得到梯度方向，向对应梯度相反方向进行规定步长的迭代搜索，找到一个函数的局部最小值。反正为梯度上升法。

具体见问题章节内的内容

**利用模型的数学形式找出目标函数，并通过求导得到梯度下降的公式**

#### 逻辑回归的优缺点

优点：数学含义上的支撑；可解释性强；工程化的需要

缺点：表达能力不强，**只对单一特征做简单加权**，无法进行特征交叉、特征筛选等高级操作，会造成信息损失。

### POLY2

在LR的基础上对特征进行**暴力组合**，其中权重系数为单独的参数，非两个向量的内积（FM改进了这里）

![POLY1](https://sunjc911.github.io/assets/images/DeepRec/POLY1.png)

#### 缺陷

采用one-hot处理类别数据，导致向量极度稀疏，特征交叉后更稀疏，导致权重缺乏有效数据进行训练，无法收敛；权重参数数量n变为n平方。

### 因子分解机FM

为了解决POLY2的缺陷。

![FM1](https://sunjc911.github.io/assets/images/DeepRec/FM1.png)

用两个向量的内积取代了POLY2单一的权重系数。具体来说，**FM为每个特征学习一个隐向量权重。在特征交叉时，使用两个特征的隐向量的内积作为交叉特征的权重**，与MF异曲同工。把MF的用户物品的隐向量拓展到了所有特征上。

参数从POLY2的n平方变为nk，k为隐向量维度，n>>k。

#### 因子分解机的优缺点

优点：很好解决数据稀疏问题。虽然丢失某些具体特征组合的精确记忆能力，但是泛化能力大幅度提高。

### 特征域感知因子分解机FFM



基于FM，引入特征域感知，使模型表达能力更强。

![FFM1](https://sunjc911.github.io/assets/images/DeepRec/FFM1.png)

每个**特征对应**的不是唯一一个隐向量，而是**一组隐向量**。

![FFM2](https://sunjc911.github.io/assets/images/DeepRec/FFM2.png)

https://blog.csdn.net/weixin_44441131/article/details/119827464

引入更多有价值信息，表达能力更强，但是参数量为nkf，复杂度为kn平方。

### GBDT+LR——特征工程模型化的开端

如果做再高维度的特征交叉会组合爆炸和复杂度过高。

Facebook利用GBDT自动进行特征筛选和组合，进而生成新的离散特征向量，再把该特征向量当作LR模型输入，预测CTR的模型结构。GBDT和LR分别**独立**训练。

![GBDTLR1](https://sunjc911.github.io/assets/images/DeepRec/GBDTLR1.png)

#### GBDT

**决策树组成的森林**，学习方法是**梯度提升**。容易过拟合，会丢失大量特征的数值信息。

具体GBDT系列见问题章节内容。

![GBDT](https://sunjc911.github.io/assets/images/DeepRec/GBDT.png)

![GBDT1](https://sunjc911.github.io/assets/images/DeepRec/GBDT1.png)

![GBDT2](https://sunjc911.github.io/assets/images/DeepRec/GBDT2.png)

#### GBDT+LR开启特征工程新趋势

GBDT+LR组合模型的提出，意味着**特征工程可以完全交由一个独立的模型**来完成，模型的输入可以是原始的特征向量，**不必在特征工程上投入过多的人工筛选和模型设计的精力**，实现真正的**端到端** (End to End) 训练。

### 大规模分段线性模型LS-PLM（混合逻辑回归MLR）——阿里

Large Scale Piece-wise Linear Model， Mixed Logistic Regression

影响力大，与3层DNN相似。

在LR的基础上采用分而治之，先对样本进行分片，再在样本分片中应用LR进行CTR预估。

在LR基础上**加入聚类**，先对全量样本进行聚类，再对每个分类LR进行CTR预估。

![MLR1](https://sunjc911.github.io/assets/images/DeepRec/MLR1.png)

![MLR2](https://sunjc911.github.io/assets/images/DeepRec/MLR2.png)

#### 优势

端到端的非线性学习能力；模型的稀疏性强，建模引入L1、L2和1范数，使得模型具有较高的稀疏度，部署更加轻量级，效率高。

#### whyL1范数比L2范数更容易产生稀疏解？

左红L2，右红L1，蓝色为损失函数曲线。

![MLR3](https://sunjc911.github.io/assets/images/DeepRec/MLR3.png)

b图在顶点处相交，除了相切处维度不为零，其他都为0，所以更稀疏。

#### 从DL的角度审视LS-PLM

LS-PLM可以看作一个加入了注意力 (Attention) 机制的三层神经网络模型，其中输入层是样本的特征向量，中间层是由 m 个神经元组成的隐层，其中 m 是分片的个数，对于一个CTR预估问题，LS-PLM的最后一层自然是由单一神经元组成的输出层。

那么，注意力机制又是在哪里应用的呢? 其实是在隐层和输出层之间，神经元之间的权重是由分片函数得出的注意力得分来确定的。也就是说，样本属于哪个分片的概率就是其注意力得分。

### 总结

![21](https://sunjc911.github.io/assets/images/DeepRec/21.png)

![22](https://sunjc911.github.io/assets/images/DeepRec/22.png)

## 第三章 深度学习在推荐系统的应用

表达能力更强，结构更加灵活

![DL](https://sunjc911.github.io/assets/images/DeepRec/DL.png)

### AutoRec——单隐层神经网络推荐模型

自编码器与CF结合

#### 基本原理

**利用CF的共现矩阵，完成用户或物品向量的自编码**。再利用自编码的结果预估评分再排序。

##### 自编码器

指能够完成数据“自编码”的模型。将输入**转换成向量的形式**表达。假设其数据向量为 r，自编码器的作用是将向量r作为输入，通过自编码器后，得到的输出向量尽量接近其本身。

![AR1](https://sunjc911.github.io/assets/images/DeepRec/AR1.png)

一般来说，重建函数的参数数量远小于输入向量的维度数量，因此自编码器相当于完成了**数据压缩和降维**的工作。

经过自编码器生成的输出向量，由于经过了自编码器的“泛化”过程，**不会完全等同于输入向量**，也因此**具备了一定的缺失维度的预测能力**，这也是自编码器能用于推荐系统的原因。

#### AutoRec模型的结构

单隐层NN

![AR2](https://sunjc911.github.io/assets/images/DeepRec/AR2.png)

![AR3](https://sunjc911.github.io/assets/images/DeepRec/AR3.png)

##### 什么是神经元、神经网络、梯度反向传播

神经元（感知机）与神经网络

![AR4](https://sunjc911.github.io/assets/images/DeepRec/AR4.png)

![AR5](https://sunjc911.github.io/assets/images/DeepRec/AR5.png)

前向传播和反向传播

前向传播的目的是在当前网络参数的基础上得到模型对输入的预估值，也就是常说的模型推断过程。在得到预估值之后，就可以利用损失函数(Loss Function) 的定义计算模型的损失。

利用梯度下降法反向更新权重。利用求导过程中的链式法则。

#### 基于AutoRec模型的推荐过程

![AR6](https://sunjc911.github.io/assets/images/DeepRec/AR6.png)

#### 优缺点

优点：使用NN，有一定泛化和表达能力。

缺点：过于简单导致表达能力不足。

与Word2vec完全一致，但优化目标和训练方法不同。

### Deep Crossing——经典的深度学习框架

微软的深度学习框架在Rec中的完整应用。

完整地解决了从特征工程、稀疏向量稠密化、多层神经网络进行优化目标拟合等一系列深度学习在推荐系统中的应用问题，为后续的研究打下了良好的基础。

#### 应用场景

Bing搜索引擎中返回相关广告，尽可能增加其点击率。准确预测广告点击率并作为广告排序的指标之一，是非常重要的，且是该模型的优化目标。

分为三类特征。可被处理为one/multi-hot向量的类别特征；计数特征；进一步处理的特征。

![DR](https://sunjc911.github.io/assets/images/DeepRec/DR.png)

处理完特征后进行CTR预估。

深度学习网络的特点是可以根据需求灵活地对网络结构进行调整，从而达成**从原始特征向量到最终的优化目标的端到端的训练**目的。

#### 网络结构

为完成端到端的训练，Deep Crossing模型要在其内部网络中解决如下问题。

(1)离散类特征编码后过于稀疏，不利于直接输入神经网络进行训练，如何解决**稀疏特征向量稠密化**的问题。
(2) 如何解决**特征自动交叉组合**的问题
(3) 如何在输出层中**达成**问题设定的**优化目标**
Deep Crossing模型分别设置了不同的神经网络层来解决上述问题。如图3-6所示，其网络结构主要包括4层-Embedding层、Stacking层、Multiple Residual Units层和Scoring层。

![DR1](https://sunjc911.github.io/assets/images/DeepRec/DR1.png)

![DR2](https://sunjc911.github.io/assets/images/DeepRec/DR2.png)

Stacking层: Stacking层(堆叠层) 的作用比较简单，是把不同的Embedding特征和数值型特征**拼接**在一起，形成新的包含全部特征的特征向量，该层通常也被称为连接(concatenate )层。

![DR3](https://sunjc911.github.io/assets/images/DeepRec/DR3.png)

Scoring层: Scoring层作为输出层，就是为了拟合优化目标而存在的。对于**CTR 预估这类二分类问题**，Scoring 层往往使用的是逻辑回归模型，而对于图像分类等多分类问题Scoring层往往采用softmax模型。

##### 残差网络及特点

残差单元（Residual Unit）组成

![DR4](https://sunjc911.github.io/assets/images/DeepRec/DR4.png)

![DR5](https://sunjc911.github.io/assets/images/DeepRec/DR5.png)

传统MLP越深就过拟合，测试集表现越差。RU可以减少过拟合。

NN越深，**梯度消失**越严重。梯度消失现象是指在梯度反向传播过程中，越靠近输入端，梯度的幅度越小，参数收敛的速度越慢。为了解决这个问题，残差单元使用了 **ReLU 激活函数取代原来的sigmoid 激活函数**。此外，输入向量**短路**相当于直接把梯度毫无变化地传递到下一层，这也使残差网络的收敛速度更快。

#### DR对特征交叉方法的革命

将全部特征交叉的任务交给模型。只需要调整NN的深度进行特征之间的“深度交叉”。

### NeuralCF模型——CF与深度学习的结合

何向南YYDS

#### 深度学习视角重新审视矩阵分解模型

隐向量为Embedding，内积获得得分为输出层

#### 结构

多层神经网络+输出层 代替 MF中的内积操作。收益直观：一是让用户向量和物品向量做更**充分的交叉**，得到更多有价值的特征组合信息: 二是**引入更多的非线性特征**，让模型的表达能力更强。

![NCF](https://sunjc911.github.io/assets/images/DeepRec/NCF.png)

框住的是互操作层，将内积改为其他操作称为”广义矩阵分解“模型GMF。

NCF再进行stacking，使得模型有更强的特征组合和非线性能力。

![NCF1](https://sunjc911.github.io/assets/images/DeepRec/NCF1.png)

##### Softmax函数

作为输出层解决多分类问题的目标拟合问题。

![NCF2](https://sunjc911.github.io/assets/images/DeepRec/NCF2.png)

![NCF3](https://sunjc911.github.io/assets/images/DeepRec/NCF3.png)

往往与交叉熵损失函数一起用：

![NCF4](https://sunjc911.github.io/assets/images/DeepRec/NCF4.png)

![NCF5](https://sunjc911.github.io/assets/images/DeepRec/NCF5.png)

#### NCF优缺点

优点：隐向量改为Embedding，利用互操作特征交叉，灵活拼接。

缺点：基于CF，没有其他类型特征，浪费了其他有用信息，没有对互操作做进一步探究。

NN理论上能拟合任意函数。模型不是越复杂越好，要防止过拟合。复杂模型往往需要更多数据和更长训练时间。

### PNN——加强特征交叉能力

引入乘积层，给出特征交互方式的几种设计思路，用于解决CTR预估。

#### 网络架构

用product layer乘积层代替Deep Crossing中的stacking层，更加针对性地获取特征之间的交叉信息。

![PNN](https://sunjc911.github.io/assets/images/DeepRec/PNN.png)

可以输入更多特征。而不是NCF那样的。

#### Product层的多种特征交叉方式

该层由内积操作（图中z部分）和外积（outer product）操作（图中p部分）。

内积操作为经典的向量内积运算。

![PNN1](https://sunjc911.github.io/assets/images/DeepRec/PNN1.png)

![PNN2](https://sunjc911.github.io/assets/images/DeepRec/PNN2.png)

#### PNN优缺点

优点：强调Embedding向量之间的交叉方式是多样化的。更容易捕获特征的交叉信息。

缺点：外积中为效率而使用平**均池化会忽略原始特征中包含的有价值信息**。

如何使得特征交叉方式更高效，后续模型给出解决方案。

### Wide&Deep模型——记忆能力和泛化能力的综合

谷歌，单层Wide和多层Deep组成的混合模型。wide让模型具有较强的”记忆能力（memorization）“，deep使模型具有”泛化能力（generalization）“。正是这样的结构特点，使模型兼具了逻辑回归和深度神经网络的优点--**能够快速处理并记忆大量历史行为特征，并且具有强大的表达能力**，不仅在当时迅速成为业界争相应用的主流模型，而且衍生出了大量以Wide&Deep模型为基础结构的混合模型，**影响力一直延续到至今**。

#### 模型的记忆能力和泛化能力

“记忆能力”可以被理解为**模型直接学习并利用历史数据中物品或者特征的“共现频率”的能力**。一般来说，协同过滤、逻辑回归等简单模型有较强的“记忆能力”。由于这类模型的结构简单，原始数据往往可以直接影响推荐结果，产生类似于“如果点击过A，就推荐B”这类规则式的推荐，这就相当于模型直接记住了历史数据的分布特点，并利用这些记忆进行推荐。

![WD](https://sunjc911.github.io/assets/images/DeepRec/WD.png)

“泛化能力”可以被理解为**模型传递特征的相关性，以及发掘稀疏甚至从未出现过的稀有特征与最终标签相关性的能力**。矩阵分解比协同过滤的泛化能力强，因为矩阵分解引入了隐向量这样的结构，使得数据稀少的用户或者物品也能生成隐向量，从而获得有数据支撑的推荐得分，这就是非常典型的**将全局数据传递到稀疏物品上，从而提高泛化能力**的例子。再比如，深度神经网络通过特征的多次自动组合，可以深度发掘数据中潜在的模式，即使是非常稀疏的特征向量输入，也能得到较稳定平滑的推荐概率，这就是简单模型所缺乏的“泛化能力”

#### 模型结构

![WD1](https://sunjc911.github.io/assets/images/DeepRec/WD1.png)

Wide& Deep模型把单输入层的Wide部分与由Embedding层和多隐层组成的Deep部分连接起来，一起输入最终的输出层。单层的Wide部分善于处理大量稀疏的id类特征: Deep部分利用神经网络表达能力强的特点，进行深层的特征交叉，挖掘藏在特征背后的数据模式。最终，利用逻辑回归模型，输出层将 Wide部分和Deep部分组合起来，形成统一的模型。

![WD2](https://sunjc911.github.io/assets/images/DeepRec/WD2.png)

![WD3](https://sunjc911.github.io/assets/images/DeepRec/WD3.png)

#### Deep&Cross（DCN）——WD的进化

使用Cross网络代替Wide部分

![WD4](https://sunjc911.github.io/assets/images/DeepRec/WD4.png)

Corss网络的目的是**增加特征之间的交互力度**，使用多层交叉层（Cross layer）对输入向量进行特征交叉。

![WD5](https://sunjc911.github.io/assets/images/DeepRec/WD5.png)

二阶部分类似PNN的外积。

![WD6](https://sunjc911.github.io/assets/images/DeepRec/WD6.png)

#### Wide&Deep影响力

DeepFM、NFM等模型可以看作其延伸。

成功的关键：(1) **抓住**了业务问题的**本质**特点，能够**融合**传统模型记忆能力和深度学习模型泛化能力的**优势**。
(2) 模型的**结构并不复杂**，比较容易在工程上实现、训练和上线，这加速了其在业界的推广应用。

### FM与深度学习模型的结合

#### FNN——用FM的隐向量完成Embedding层初始化

![FNN](https://sunjc911.github.io/assets/images/DeepRec/FNN.png)

FNN在哪里与FM模型进行了结合呢？关键在于Embedding层的改进。

参数初始化如果运气不好极端，会导致Embedding层收敛慢。

##### 为什么Embedding层的收敛速度往往很慢

1.参数量巨大。权重占据整个网络权重的大部分。

2.输入向量过于稀疏。随机梯度下降只更新与非零特征相连的Embedding层权重。

针对此问题。FNN的解决思路是用FM模型训练好的各特征隐向量初始化Embedding层的参数。相当于引入先验，加速收敛过程。FM公式：

![FNN1](https://sunjc911.github.io/assets/images/DeepRec/FNN1.png)

w为1阶权重，v为二阶隐向量。

![FNN2](https://sunjc911.github.io/assets/images/DeepRec/FNN2.png)

![FNN3](https://sunjc911.github.io/assets/images/DeepRec/FNN3.png)

#### DeepFM——用FM代替Wide部分

![DFM](https://sunjc911.github.io/assets/images/DeepRec/DFM.png)

![DFM1](https://sunjc911.github.io/assets/images/DeepRec/DFM.png)

#### NFM——FM的神经网络化尝试

FM和FFM最多到二阶特征交叉，再高就组合爆炸，限制表达能力。能不能用NN来改进FM。

NFM用一个表达能力更强的函数替代原FM中二阶隐向量内积的部分。

![NFM](https://sunjc911.github.io/assets/images/DeepRec/NFM.png)

用下面的网络结构作为f（x）函数替代二阶

![NFM1](https://sunjc911.github.io/assets/images/DeepRec/NFM1.png)

![NFM2](https://sunjc911.github.io/assets/images/DeepRec/NFM2.png)

肚脐眼为element-wise product元素积，即对应元素相乘。

![NFM3](https://sunjc911.github.io/assets/images/DeepRec/NFM3.png)

#### 基于FM的深度学习模型的优缺点

优点：FNN、DeepFM、NFM都在经典多层NN的基础上加入了针对性的特征交叉操作，让模型具备更强的非线性表达能力。

缺点：特征工程的思路几乎穷尽，模型提升空间小。

### 注意力机制的应用

#### AFM——引入注意力机制的FM

NFM模型的延续。在NFM模型中，对特征交叉池化层后的特征向量进行”加和池化“，会“一视同仁”地对待所有交叉特征，不考虑不同特征对结果的影响程度，消解了有价值的信息。

AFM模型引入注意力机制是通过在特征交叉层和最终的输出层之间加入注意力网络实现。注意力网络的作用是为每一个交叉特征提供权重，也就是注意力得分。

![AFM](https://sunjc911.github.io/assets/images/DeepRec/AFM.png)

![AFM1](https://sunjc911.github.io/assets/images/DeepRec/AFM1.png)

![AFM2](https://sunjc911.github.io/assets/images/DeepRec/AFM1.png)

#### DIN——引入注意力机制的深度学习网络

更具有业务气息。应用于阿里巴巴的电商广告推荐。

![DIN](https://sunjc911.github.io/assets/images/DeepRec/DIN.png)

![DIN1](https://sunjc911.github.io/assets/images/DeepRec/DIN1.png)

![DIN2](https://sunjc911.github.io/assets/images/DeepRec/DIN2.png)

如果留意图3-24中的红线，可以发现商铺id只跟用户历史行为中的商铺 id序列发生作用，商品 id只跟用户的商品 id序列发生作用，因为注意力的轻重更应该由同类信息的相关性决定。
DIN模型与基于 FM的 AFM模型相比，是一次更典型的改进深度学习网络的尝试，而且由于出发点是具体的业务场景，也给了推荐工程师更多实质性的启发。

### DIEN——序列模型与推荐系统的结合

DIN的进化版本。创新在于用序列模型模拟了用户兴趣的进化过程。

#### DIEN的进化动机

历史行为都是一个随时间排序的序列。一定存在强或弱的前后依赖关系。AFM和DIN获得的注意力得分与时间无关，即序列无关。

序列信息的重要性：（1）加强最近行为对下次行为预测的影响。（2）序列模型能够学习到购买趋势的信息。

#### DIEN模型的架构

![DIEN](https://sunjc911.github.io/assets/images/DeepRec/DIEN.png)

![DIEN1](https://sunjc911.github.io/assets/images/DeepRec/DIEN1.png)

![DIEN2](https://sunjc911.github.io/assets/images/DeepRec/DIEN2.png)

#### 兴趣抽取层的结构

基本结构GRU门循环单元。相比于传统的RNN循环神经网络和LSTM长短期记忆网络。GRU解决了RNN的梯度消失问题。与LSTM相比，GRU的参数量更少，训练收敛速度更快。

![DIEN3](https://sunjc911.github.io/assets/images/DeepRec/DIEN3.png)

![DIEN4](https://sunjc911.github.io/assets/images/DeepRec/DIEN4.png)

#### 兴趣进化层的结构

最大特点：加入注意力机制。

![DIEN5](https://sunjc911.github.io/assets/images/DeepRec/DIEN5.png)

#### 序列模型对推荐系统的启发

具备强大的序列表达能力，非常适合预估用户经过一系列行为后的下一次动作。

模型复杂度较高，训练复杂度较高，服务过程中延迟大，增大其上线的难度，需要在工程上着重优化。

### 强化学习与推荐系统的结合

### 总结

![3o1](https://sunjc911.github.io/assets/images/DeepRec/3o1.png)

![3o2](https://sunjc911.github.io/assets/images/DeepRec/3o2.png)

![3o3](https://sunjc911.github.io/assets/images/DeepRec/3o3.png)

## 第四章 Embedding技术在推荐系统中的应用

### 什么是Embedding

用一个低维稠密的向量“表示”一个对象。“表示”意味着 Embedding向量能够表达相应对象的某些特征，同时向量之间的距离反映了对象之间的相似性。

#### Embedding的重要性

1.推荐中大量使用独热编码、id编码，导致极度稀疏，深度学习的结构特点不利于稀疏特征向量的处理。用Embedding层将高维稀疏转为低维稠密。所以为DL的基础操作。

2.本身就是重要的特征向量。表达能力更强，尤其是Graph Embedding技术提出后。

3.基于Embedding的相似度内积更适用于召回。

### Word2Vec——经典的Embedding方法

谷歌，使得词向量从NLP推广到搜广推领域。

#### 什么是Word2Vec

![WV](https://sunjc911.github.io/assets/images/DeepRec/WV.png)

T为句子长度，选择2c+1的滑动窗口。

![WV1](https://sunjc911.github.io/assets/images/DeepRec/WV1.png)

什么是输出向量表达和输入向量表达？

![WV2](https://sunjc911.github.io/assets/images/DeepRec/WV2.png)

![WV3](https://sunjc911.github.io/assets/images/DeepRec/WV3.png)

![WV4](https://sunjc911.github.io/assets/images/DeepRec/WV4.png)

#### Word2Vec的“负采样”训练方法

![WV5](https://sunjc911.github.io/assets/images/DeepRec/WV5.png)

![WV6](https://sunjc911.github.io/assets/images/DeepRec/WV6.png)

#### Word2Vec对Embedding技术的意义

牛逼。从另一个角度看，在 Word2vec 的研究中提出的模型结构、目标函数、负采样方法及负采样中的目标函数，在后续的研究中被重复使用并被屡次优化。掌握Word2vec 中的每个细节成了研究 Embedding 的基础。从这个意义上讲，熟练掌握本节内容非常重要。

### Item2vec——Word2vec在推荐系统领域的推广

Word2vec对词序列进行embedding，那么rec中可以对用户购买序列进行embedding。

#### 基本原理

利用用户向量和物品向量的相似性，可以直接在推荐系统的召回层快速得到候选集合，或在排序层直接用于最终推荐列表的排序。

![IV](https://sunjc911.github.io/assets/images/DeepRec/IV.png)

#### “广义”Item2vec

任何生成物品向量的方法都可以成为Item2vec。如双塔模型。

![IV1](https://sunjc911.github.io/assets/images/DeepRec/IV1.png)

![IV2](https://sunjc911.github.io/assets/images/DeepRec/IV2.png)

#### 优缺点

优点：推广Word2vec，大大拓展应用场景。广义上的Item2vec模型其实是物品向量化方法的统称。

缺点：只能利用序列数据。

### Graph Embedding——引入更多结构信息的图嵌入技术

![GE](https://sunjc911.github.io/assets/images/DeepRec/GE.png)

Graph Embedding是一种对图结构中的节点进行Embedding编码的方法。最终生成的节点 Embedding 向量一般包含图的结构信息及附近节点的局部相似性信息。

#### Deep Walk——基础的Graph Embedding方法

在由**物品组成的图结构**上进行随机游走，然后产生大量物品序列，然后作为样本输入到Word2vec进行训练，得到物品的Embedding。

![GE1](https://sunjc911.github.io/assets/images/DeepRec/GE1.png)

![GE2](https://sunjc911.github.io/assets/images/DeepRec/GE2.png)

#### Node2vec——同质性和结构性的权衡

基于Deep Walk，它通过调整随机游走权重的方法使Graph Embedding的结果更倾向于体现网络的司质性 (homophily) 或结构性 (structural equivalence) 。

具体地讲，网络的“同质性”指的是距离相近节点的Embedding应尽量近似，如图 4-8所示，节点u 与其相连的节点 s1、S2、S3、S4的 Embedding 表达应该是接近的，这就是网络“同质性”的体现。

“结构性”指的是结构上相似的节点的 Embedding 应尽量近似，图 4-8 中节点 U 和节点 s6都是各自局域网络的中心节点，结构上相似，其 Embedding 的表达也应该近似，这是“结构性”的体现。

![GE3](https://sunjc911.github.io/assets/images/DeepRec/GE3.png)

![GE4](https://sunjc911.github.io/assets/images/DeepRec/GE4.png)

![GE5](https://sunjc911.github.io/assets/images/DeepRec/GE5.png)

![GE6](https://sunjc911.github.io/assets/images/DeepRec/GE6.png)

Node2vec 所体现的网络的同质性和结构性在推荐系统中可以被很直观的解释。同质性相同的物品很可能是同品类、同属性，或者经常被一同购买的商品，而结构性相同的物品则是各品类的爆款、各品类的最佳凑单商品等拥有类似趋势或者结构性属性的商品。毫无疑问，二者在推荐系统中都是非常重要的特征表达。由于 Node2vec 的这种灵活性，以及发掘不同图特征的能力，甚至可以把不同Node2vec生成的偏向“结构性”的Embedding结果和偏向“同质性”的Embedding结果共同输入后续的深度学习网络，以保留物品的不同图特征信息。

#### EGES——阿里巴巴的综合性Graph Embedding方法

在Deep Walk生成的Graph Embedding基础上引入补充信息。

为了使冷启动的商品获得合理的初始Embedding，引入补充信息side information来丰富Embedding信息的来源。

生成Graph Embedding的第一步是生成物品关系图，通过用户行为序列可以生成物品关系图，也可以利用“相同属性”“相同类别”等信息建立物品之间的边，生成基于内容的知识图谱。而基于知识图谱生成的物品向量可以被称为补充信息 Embedding向量。当然，根据补充信息类别的不同，可以有多个补充信息Embedding向量。

![GE7](https://sunjc911.github.io/assets/images/DeepRec/GE7.png)

![GE8](https://sunjc911.github.io/assets/images/DeepRec/GE8.png)

### Embedding与深度学习推荐系统的结合

作为深度学习推荐系统不可分割的一部分，Embedding技术主要应用在如下三个方向。
(1) 在深度学习网络中作为Embedding层，完成从**高维稀疏特征向量到低维稠密**特征向量的转换。
(2)作为预训练的Embedding特征向量，与其他特征向量连接后，一同输入深度学习网络进行训练。
(3) 通过计算用户和物品的Embedding相似度，Embedding可以直接作为推荐系统的召回层或者召回策略之一。

#### 深度学习网络中的Embedding层

![GE9](https://sunjc911.github.io/assets/images/DeepRec/GE9.png)

![GE10](https://sunjc911.github.io/assets/images/DeepRec/GE10.png)

#### Embedding预训练方法

为解决Embedding层训练开销大的问题。

FNN模型就是采用的预训练，利用FM模型的各特征隐向量作为初始化Embedding权重，从而加快网络收敛速度。

若想更快收敛，可以固定Embedding权重，仅更新上层网络的权重，这是更彻底的预训练方法。

再延伸一下，Embedding的本质是建立高维向量到低维向量的映射，而“映射”的方法并不局限于神经网络，可以是任何异构模型。例如，2.6节介绍的GBDT+LR组合模型，其中GBDT部分在本质上就是进行了一次Embedding操作，利用GBDT模型完成Embedding预训练，再将Embedding输入单层神经网络(即逻辑回归) 进行CTR预估

通常，Graph Embedding的训练只能独立于推荐模型进行，所以预训练受青睐。

诚然，将Embedding过程与深度神经网络的训练过程割裂会损失一定的信息，但训练过程的独立也带来了训练灵活性的提升。举例来说，物品或用户的Embedding是比较稳定的(因为用户的兴趣、物品的属性不可能在几天内发生巨大的变化)，Embedding的训练频率其实不需要很高，甚至可以降低到周的级别，但上层神经网络为了尽快抓住最新的数据整体趋势信息，往往需要高频训练甚至实时训练。使用不同的训练频率更新Embedding模型和神经网络模型，是训练开销和模型效果二者之间权衡后的最优方案。

#### Embedding作为推荐系统召回层的方法

Embedding自身表达能力的增强使得直接利用Embedding生成推荐列表成了可行的选择。因此，利用Embedding向量的相似性，将Embedding作为推荐系统召回层的方案逐渐被推广开来。其中，YouTube推荐系统召回层(如图4-14所示) 的解决方案是典型的利用Embedding进行候选物品召回的做法。

![GE11](https://sunjc911.github.io/assets/images/DeepRec/GE11.png)

![GE12](https://sunjc911.github.io/assets/images/DeepRec/GE12.png)

![GE13](https://sunjc911.github.io/assets/images/DeepRec/GE13.png)