---
title: 深度学习推荐系统
description: 
date: 2023-04-22
categories:
 - Rec面试
tags:
 - Rec
 - 面试
excerpt_separator: <!--more--> 

---

**加油**

<!--more-->

## 问题

[常见损失函数](https://blog.csdn.net/yanyuxiangtoday/article/details/119788949)

[L1L2正则化](https://zhuanlan.zhihu.com/p/137073968)

[梯度下降求导_LR](https://blog.csdn.net/qq_35890572/article/details/107123956)

[极大似然估计](https://lulaoshi.info/machine-learning/linear-model/maximum-likelihood-estimation.html#%E6%A6%82%E7%8E%87)

[GBDT系列](https://blog.csdn.net/XiaoYi_Eric/article/details/80167968)

## 第一章 互联网的增长引擎——推荐系统

### 推荐系统的作用与意义

用户角度: 推荐系统解决在“信息过载”的情况下，用户如何高效获得感兴趣信息的问题。

公司角度:推荐系统解决产品能够最大限度地吸引用户、留存用户、增加用户黏性提高用户转化率的问题，从而达到公司商业目标连续增长的目的。需要注意的是，设计推荐系统的最终目标是达成公司的商业目标、增加公司收益这应是推荐工程师站在公司角度考虑问题的出发点。

### 推荐系统的架构

(1) 互联网企业的核心需求是“增长”，而推荐系统正处在“增长引擎”的核心位置(2) 推荐系统要解决的“用户痛点”是用户如何在“信息过载”的情况下高效地获得感兴趣的信息。

![1](https://sunjc911.github.io/assets/images/DeepRec/1.png)

### 推荐系统的模型部分

“**召回层**”一般利用高效的召回规则、算法或简单的模型，快速从海量的候选集中召回用户可能感兴趣的物品。
“**排序层（重点与研究重心）**”利用排序模型对初筛的候选集进行精排序。
“补充策略与算法层”，也被称为“再排序层**”，可以在将推荐列表返回用户之前，为兼顾结果的“多样性”“流行度”“新鲜度”等指标，结合一些补充的策略和算法对推荐列表进行定的调整，最终形成用户可见的推荐列表。

**模型权重**：**离线训练**的特点是可以利用全量样本和特征，使模型逼近全局最优点;**在线更新**则可以准实时地“消化”新的数据样本，更快地反映新的数据变化趋势，满足模型实时性的需求。

**评估模型效果**：离线评估和线上A/B测

## 第二章 前深度学习时代的重要算法

协同过滤、逻辑回归、因子分解机等传统推荐模型仍然凭借其可解释性强、硬件环境要求低易于快速训练和部署。

传统推荐模型是深度学习推荐模型的基础。

![2](https://sunjc911.github.io/assets/images/DeepRec/2.png)

### 协同过滤CF

“协同过滤”就是协同大家的反馈、评价和意见一起对海量的信息进行过滤从中筛选出目标用户可能感兴趣的信息的推荐过程。

![3](https://sunjc911.github.io/assets/images/DeepRec/3.png)

#### UserCF

##### 用户相似度计算

###### 余弦相似度

余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小。余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，这就叫"余弦相似性"。

![cos](https://sunjc911.github.io/assets/images/DeepRec/cos.png)

###### 皮尔逊相关系数

相比余弦相似度，皮尔逊相关系数通过使用用户平均分对各独立评分进行修正，减小了用户评分偏置的影响。

![p](https://sunjc911.github.io/assets/images/DeepRec/p.png)

![p2](https://sunjc911.github.io/assets/images/DeepRec/p2.png)

理论上，任何合理的“向量相似度定义方式”都可以作为相似用户计算的标准。

###### 最终结果的排序

![4](https://sunjc911.github.io/assets/images/DeepRec/4.png)

###### UserCF缺点

（1）用户数远大于物品数，用户相似度**矩阵开销大**，用户数增长时矩阵空间复杂度以n平方的速度增长，存储系统难以承受。

（2）用户历史数据**向量非常稀疏**，不适用于正反馈获取困难的场景。

#### ItemCF

构建物品相似度矩阵，相似度计算方法与UserCF相同。

···

(4) 利用物品相似度矩阵，针对目标用户历史行为中的正反馈物品，找出相似的Top k个物品，组成相似物品集合。
(5) 对相似物品集合中的物品，利用相似度分值进行排序，生成最终的推荐列表。在第5步中，如果一个物品与多个用户行为历史中的正反馈物品相似，那么该物品最终的相似度应该是多个相似度的累加，如(式2-5) 所示。·

![5](https://sunjc911.github.io/assets/images/DeepRec/5.png)

#### UserCF和ItemCF的应用场景

UserCF：社交、发现热点、跟踪热点、新闻

ItemCF：电商、视频推荐

#### 协同过滤的优缺点

优点：直观、可解释性强

缺点：不具备较强泛化能力，产生头部效应（热门商品容易和大量物品产生相似，冷门商品向量稀疏则反过来很少被推荐），处理稀疏向量的能力弱

### 矩阵分解MF

针对CF的头部效应明显、泛化能力较弱的问题。在CF**共现矩阵的基础**上，加入**隐向量**的概念，加强模型处理稀疏矩阵的能力，针对性地解决了协同过滤存在的主要问题。

矩阵分解算法则期望为每一个用户和视频生成一个隐向量，将用户和视频定位到隐向量的表示空间上，距离相近的用户和视频表明兴趣特点接近，在推荐过程中，就应该把距离相近的视频推荐给目标用户。任意用户与物品之间都可以得到预测分。

![6](https://sunjc911.github.io/assets/images/DeepRec/6.png)

用户和物品的隐向量通过CF的共现矩阵得到。

![MF](https://sunjc911.github.io/assets/images/DeepRec/MF.png)

mxn分解为mxk和kxn。k小隐向量小泛化能力高，反之则反。

![MF2](https://sunjc911.github.io/assets/images/DeepRec/MF2.png)

#### 矩阵分解的求解过程

##### 特征值分解ED

只适用于方阵，这里不行。

##### 奇异值分解SVD

![MF3](https://sunjc911.github.io/assets/images/DeepRec/MF3.png)

完美解决MF但有缺陷：

（1）要求共现矩阵稠密。共现矩阵稀疏，若要用需填充。

（2）计算复杂度O（mn平方）

##### 梯度下降GD（重要）

![GD](https://sunjc911.github.io/assets/images/DeepRec/GD.png)

##### 正则化

![reg](https://sunjc911.github.io/assets/images/DeepRec/reg.png)

q=1 L1正则化；2 L2正则化

对于加入了正则化项的损失函数来说，模型权重的值越大，损失函数越大。梯度下降是**朝着损失小的方向**发展的，因此正则化项其实是希望在尽量不影响原模型与数据集之间损失的前提下，**使模型的权重变小**，权重的减小自然会让模型的输出波动更小，从而达到**让模型更稳定**的目的。

可以加上偏差系数修正预测分数。

![7](https://sunjc911.github.io/assets/images/DeepRec/7.png)

##### 矩阵分解的优缺点

优点：泛化能力强，缓解数据稀疏问题；空间复杂度低（n+m）k；更好的扩展性和灵活性。

缺点：没发使用用户物品上下文特征。

### 逻辑回归LR

CF和MF利用相似度进行推荐。LR将推荐问题看作**分类问题**，预测正样本的概率对物品进行排序，转化为**点击率（Click Through Rate, CTR）**预估问题。综合利用用户物品上下文等特征。利用“感知机MLP”这种NN的基本单元，是深度学习的基础。

各特征加权和再施以sigmod函数

离散转为数值型特征向量，训练，得到点击物品的概率，根据概率排序进行推荐。

#### 数学形式

![LR1](https://sunjc911.github.io/assets/images/DeepRec/LR1.png)

![LR2](https://sunjc911.github.io/assets/images/DeepRec/LR2.png)

![LR3](https://sunjc911.github.io/assets/images/DeepRec/LR3.png)

![LR4](https://sunjc911.github.io/assets/images/DeepRec/LR4.png)

#### 权重训练——梯度下降法

目的是对目标函数求导，得到梯度方向，向对应梯度相反方向进行规定步长的迭代搜索，找到一个函数的局部最小值。反正为梯度上升法。

具体见问题章节内的内容

**利用模型的数学形式找出目标函数，并通过求导得到梯度下降的公式**

#### 逻辑回归的优缺点

优点：数学含义上的支撑；可解释性强；工程化的需要

缺点：表达能力不强，**只对单一特征做简单加权**，无法进行特征交叉、特征筛选等高级操作，会造成信息损失。

### POLY2

在LR的基础上对特征进行**暴力组合**，其中权重系数为单独的参数，非两个向量的内积（FM改进了这里）

![POLY1](https://sunjc911.github.io/assets/images/DeepRec/POLY1.png)

#### 缺陷

采用one-hot处理类别数据，导致向量极度稀疏，特征交叉后更稀疏，导致权重缺乏有效数据进行训练，无法收敛；权重参数数量n变为n平方。

### 因子分解机FM

为了解决POLY2的缺陷。

![FM1](https://sunjc911.github.io/assets/images/DeepRec/FM1.png)

用两个向量的内积取代了POLY2单一的权重系数。具体来说，**FM为每个特征学习一个隐向量权重。在特征交叉时，使用两个特征的隐向量的内积作为交叉特征的权重**，与MF异曲同工。把MF的用户物品的隐向量拓展到了所有特征上。

参数从POLY2的n平方变为nk，k为隐向量维度，n>>k。

#### 因子分解机的优缺点

优点：很好解决数据稀疏问题。虽然丢失某些具体特征组合的精确记忆能力，但是泛化能力大幅度提高。

### 特征域感知因子分解机FFM



基于FM，引入特征域感知，使模型表达能力更强。

![FMM1](https://sunjc911.github.io/assets/images/DeepRec/FMM1.png)

每个**特征对应**的不是唯一一个隐向量，而是**一组隐向量**。

![FMM2](https://sunjc911.github.io/assets/images/DeepRec/FMM2.png)

https://blog.csdn.net/weixin_44441131/article/details/119827464

引入更多有价值信息，表达能力更强，但是参数量为nkf，复杂度为kn平方。

### GBDT+LR——特征工程模型化的开端

如果做再高维度的特征交叉会组合爆炸和复杂度过高。

Facebook利用GBDT自动进行特征筛选和组合，进而生成新的离散特征向量，再把该特征向量当作LR模型输入，预测CTR的模型结构。GBDT和LR分别**独立**训练。

![GBDTLR1](https://sunjc911.github.io/assets/images/DeepRec/GBDTLR1.png)

#### GBDT

**决策树组成的森林**，学习方法是**梯度提升**。容易过拟合，会丢失大量特征的数值信息。

具体GBDT系列见问题章节内容。

![GBDT](https://sunjc911.github.io/assets/images/DeepRec/GBDT.png)

![GBDT1](https://sunjc911.github.io/assets/images/DeepRec/GBDT1.png)

![GBDT2](https://sunjc911.github.io/assets/images/DeepRec/GBDT2.png)

#### GBDT+LR开启特征工程新趋势

GBDT+LR组合模型的提出，意味着**特征工程可以完全交由一个独立的模型**来完成，模型的输入可以是原始的特征向量，**不必在特征工程上投入过多的人工筛选和模型设计的精力**，实现真正的**端到端** (End to End) 训练。

### 大规模分段线性模型LS-PLM（混合逻辑回归MLR）——阿里

Large Scale Piece-wise Linear Model， Mixed Logistic Regression

影响力大，与3层DNN相似。

在LR的基础上采用分而治之，先对样本进行分片，再在样本分片中应用LR进行CTR预估。

在LR基础上**加入聚类**，先对全量样本进行聚类，再对每个分类LR进行CTR预估。

![MLR1](https://sunjc911.github.io/assets/images/DeepRec/MLR1.png)

![MLR2](https://sunjc911.github.io/assets/images/DeepRec/MLR2.png)

#### 优势

端到端的非线性学习能力；模型的稀疏性强，建模引入L1、L2和1范数，使得模型具有较高的稀疏度，部署更加轻量级，效率高。

#### whyL1范数比L2范数更容易产生稀疏解？

左红L2，右红L1，蓝色为损失函数曲线。

![MLR3](https://sunjc911.github.io/assets/images/DeepRec/MLR3.png)

b图在顶点处相交，除了相切处维度不为零，其他都为0，所以更稀疏。

#### 从DL的角度审视LS-PLM

LS-PLM可以看作一个加入了注意力 (Attention) 机制的三层神经网络模型，其中输入层是样本的特征向量，中间层是由 m 个神经元组成的隐层，其中 m 是分片的个数，对于一个CTR预估问题，LS-PLM的最后一层自然是由单一神经元组成的输出层。

那么，注意力机制又是在哪里应用的呢? 其实是在隐层和输出层之间，神经元之间的权重是由分片函数得出的注意力得分来确定的。也就是说，样本属于哪个分片的概率就是其注意力得分。

### 总结

![21](https://sunjc911.github.io/assets/images/DeepRec/21.png)

![22](https://sunjc911.github.io/assets/images/DeepRec/22.png)